{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3dedef-fb90-4732-a3b3-b366d08f9a9f",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"escudo_uach.png\" alt=\"Escudo UACh\" width=\"200\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "# Análisis comparativo de bandas sonoras\n",
    "## Proyecto de ACUS 220 Acústica Computacional con Python\n",
    "## Primer Hito de Entrega\n",
    "**Integrantes:** Patricio Cuevas, Felipe Henriquez, Daniela Huenumán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947562ae-2356-482e-a288-0f2c337b9d01",
   "metadata": {},
   "source": [
    "## Objetivo General\n",
    "Diseñar un modelo en Python que permita caracterizar y comparar la intención emocional de bandas sonoras de videojuegos mediante análisis espectral y extracción de atributos acústicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee6f26-cfde-4192-9e90-bcbd0666b875",
   "metadata": {},
   "source": [
    "## Objetivos Específicos\n",
    "\n",
    "3. Los objetivos específicos:\n",
    "\n",
    "1) Recolectar y segmentar muestras de audio de bandas sonoras de videojuegos seleccionados (Minecraft, Stardew Valley, Undertale) usando herramientas como yt-dlp y pydub, además obtener inmediatamente características de los audios con Spotify Web API.\n",
    "2) Extraer atributos acústicos relevantes (MFCCs, centroide espectral, contraste, tempo, etc.) mediante librerías como Librosa y Essentia.\n",
    "3) Analizar la relación entre atributos espectrales y emociones percibidas, utilizando modelos de valencia y arousal para categorizar la intención emocional de cada pista.\n",
    "4) Comparar las bandas sonoras seleccionadas en términos de instrumentación, armonía y distribución espectral, identificando patrones distintivos entre juegos.\n",
    "5) Visualizar los resultados mediante espectrogramas, mapas de instrumentación y diagramas de flujo armónico, facilitando la interpretación para audiencias técnicas y no técnicas.\n",
    "6) Diseñar y entrenar un modelo de clasificación supervisado (Random Forest, SVM o red neuronal simple) que reciba atributos espectrales como entrada y prediga la atmósfera emocional del fragmento de audio.\n",
    "7) Evaluar el desempeño del modelo mediante métricas como precisión, recall y matriz de confusión, y visualizar los resultados con técnicas de reducción dimensional (PCA, t-SNE) para interpretar agrupamientos emocionales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca221c-6805-4596-94e9-070cd821c63e",
   "metadata": {},
   "source": [
    "## \tEstado del Arte\n",
    "\n",
    "En el ámbito del análisis musical y de emociones, se han desarrollado diversas aproximaciones que combinan el procesamiento del lenguaje natural con técnicas de análisis de audio. A nivel textual, existen repositorios que emplean la arquitectura T5, un modelo de transformer desarrollado por Google para trabajar con letras tokenizadas y detectar patrones emocionales expresados en las canciones [1].\n",
    "\n",
    "En cuanto al componente acústico, librerías como Librosa [3] y Essentia [4] se han consolidado como herramientas de referencia para la extracción de características como los coeficientes cepstrales en las frecuencias de Mel (MFCCs), el centroide espectral, el contraste espectral y el tempo. Dichas características facilitan el análisis automatizado de grandes volúmenes de audio y han sido utilizadas en investigaciones orientadas a la clasificación musical, la detección de emociones y la segmentación de paisajes sonoros.\n",
    "\n",
    "Asimismo, existen proyectos previos que abordan la clasificación de música por género o intención emocional, así como repositorios que organizan metadatos musicales, por ejemplo, Data-warehouse-Videogames, que sistematiza información sonora asociada a videojuegos [2]  y que sirven de referencia metodológica para nuestro trabajo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190aae9-cc5c-4a89-9421-3270436a91ca",
   "metadata": {},
   "source": [
    "## \tMateriales y Métodos\n",
    "\n",
    "En el desarrollo de este proyecto utilizaremos principalmente OSTs (Original Soundtracks) de videojuegos como fuente principal de información para nuestro modelo. Estos audios serán extraídos de videos únicamente con fines de análisis académico y de investigación. Para procesar las canciones obtenidas, utilizaremos Essentia para extraer las características acústicas necesarias y la API de Spotify para obtener datos adicionales de manera más rápida y sistemática.\n",
    "\n",
    "La recolección de datos se realizará de forma manual, con el fin de seleccionar inteligentemente los OSTs y evitar que exista una sobrerrepresentación de un mismo tipo de música que genere emociones similares. Para el análisis y modelado, se implementará un modelo supervisado, entrenado con los atributos acústicos y emocionales más relevantes extraídos de los OSTs.\n",
    "\n",
    "\n",
    "\n",
    "Materiales y datos\n",
    "* Audios / música:\n",
    "\n",
    "\n",
    "    * OSTs de videojuegos: Minecraft, Stardew Valley, Undertale, Deltarune, Ultrakill (descargadas desde Spotify o YouTube).\n",
    "    * Samples de audio de 30 segundos (preprocesados con Pydub).\n",
    "\n",
    "* Bases de datos y datasets adicionales:\n",
    "    * 200k Spotify Songs Light Dataset\n",
    "    * Muse: The Musical Sentiment Dataset\n",
    "      \n",
    "    (Para entrenamiento inicial del modelo, una vez calculemos los atributos que el modelo espere cómo MFCCs, Tempo / BPM, Centroide espectral, Contraste espectral, Loudness, valence, energy, y luego entregarle estos datos cómo entrada, por dar un ejemplo)\n",
    "\n",
    "\n",
    "Librerías de Python:\n",
    "\n",
    "\n",
    "* Librosa y Essentia → extracción de características acústicas (MFCCs, tempo, spectral contrast, centroide, etc.).\n",
    "* Pydub → segmentación y manipulación de audio.\n",
    "* Scikit-learn, TensorFlow/PyTorch → modelado supervisado / redes neuronales.\n",
    "* Matplotlib, Seaborn → visualización de datos y resultados.\n",
    "\n",
    "\n",
    "**Tabla de ejemplo**\n",
    "\n",
    "| Actividad                        | Responsable(s)   | Fecha estimada |\n",
    "|----------------------------------|-----------------|----------------|\n",
    "| Revisión bibliográfica           | Grupo completo  | 10/10/2025     |\n",
    "| Recolección de datos en terreno  | Nombre1         | 20/10/2025     |\n",
    "| Procesamiento de datos en Python | Nombre2         | 30/10/2025     |\n",
    "| Entrenamiento del modelo         | Nombre3         | 15/11/2025     |\n",
    "| Redacción de informe parcial     | Grupo completo  | 25/11/2025     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5ae8c-bf2e-407b-9448-1e099fa3b4e1",
   "metadata": {},
   "source": [
    "## Referencias Bibliográficas:\n",
    "\n",
    "[1] Romero, M. (s. f.). t5-base-finetuned-emotion [modelo en Hugging Face]. Hugging Face. https://huggingface.co/mrm8488/t5-base-finetuned-emotion\n",
    "\n",
    "[2] TheGrayish. (s. f.). Data-warehouse-Videogames [Repositorio en GitHub]. GitHub. https://github.com/TheGrayish/Data-warehouse-Videogames\n",
    "\n",
    "[3] McFee, B., Raffel, C., Liang, D., Ellis, D. P. W., McVicar, M., Battenberg, E., & Nieto, O. (2015). librosa: Audio and music signal analysis in Python. In Proceedings of the 14th Python in Science Conference (pp. 18–25). https://proceedings.scipy.org/articles/Majora-7b98e3ed-003.pdf\n",
    "\n",
    "[4] Bogdanov, D., Wack, N., Gómez, E., Gulati, S., Herrera, P., Mayor, O., Roma, G., Salamon, J., Zapata, J. R., & Serra, X. (2013). Essentia: An audio analysis library for music information retrieval. En Proceedings of the 14th International Society for Music Information Retrieval Conference (ISMIR 2013) (pp. 493–498). Universitat Pompeu Fabra. https://repositori-api.upf.edu/api/core/bitstreams/df79c17b-865f-4313-a440-e0d1e5cbf234/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d748307-ea44-4e51-a30e-cced014ca883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
